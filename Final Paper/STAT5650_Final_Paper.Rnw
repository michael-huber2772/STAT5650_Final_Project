\documentclass[12pt,letterpaper,final]{article}

\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{wasysym}

\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.15in}
%\setlength{\topmargin}{0.5in}
\setlength{\textheight}{22cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parskip}{5pt plus 2pt minus 3pt}

\def\thefootnote{\fnsymbol{footnote}}
\setcounter{footnote}{1}

\renewcommand{\baselinestretch}{1.2}
\renewcommand{\labelenumi}{(\roman{enumi})}

\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}
\renewcommand{\floatpagefraction}{1.0}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}

% to get nice proofs ...
\newcommand{\qedsymb}{\mbox{ }~\hfill~{\rule{2mm}{2mm}}}
\newenvironment{proof}{\begin{trivlist}
\item[\hspace{\labelsep}{\bf\noindent Proof: }]
}{\qedsymb\end{trivlist}}


\newfont{\msymb}{cmsy10 scaled 1000}

\def\nullset{\mbox{\O}}
\def\R{{I\!\!R}}
\def\C{{I\!\!\!\!C}}
\def\N{{I\!\!N}}

\def\P{\mbox{\msymb P}}


%\parskip 0.1in
\pagenumbering{arabic}    %  Start using 1,2,... as page numbers.
\pagestyle{plain}         %  Page numbers in middle bottom of page.
%\setcounter{page}{80}  % XXXXXXXXXXXXXXXXX
%\setcounter{theorem}{5} % XXXXXXXXXXXXXXXXX
%\setcounter{definition}{10} % XXXXXXXXXXXXXXXXX

\parindent 0in


\begin{document}

\SweaveOpts{concordance=TRUE}

\begin{table}\centering
\begin{tabular*}{6.15in}{@{\extracolsep{\fill}}|llr|} \hline
STAT 5650 Statistical Learning and Data Mining 1 & \hspace*{0.5 in} & Spring 2020 \\
 & & \\
\multicolumn{3}{|c|}{
{\bf Names:} Greg Hoffmann, Kristen Sohm, Michael Huber, Varsha Reddy Mandadi} \\
 & & \\
\multicolumn{3}{|c|}{
{\bf Submission Date:} 04/25/2020} \\
 & & \\
\multicolumn{3}{|c|}{
Final Project: Applying Prediction Methods to Hotel Data } \\
 & & \\
\multicolumn{3}{|c|}{
210 Points --- Due Monday 04/27/2020 (via Canvas by 11:59pm)} \\
\hline
\end{tabular*}
\end{table}


~ \newpage

\begin{enumerate}

\item \underline{\bf Prediction Methods:}
After working with the data and reviewing the notes from Dr. Cutler on our project proposal
we had to edit what methods of prediction we were going to apply against the data. Below is
a list of methods that we were able to get to run against our hotel data set.

\begin{itemize}
\item Gradient Boosting Machines (GBM)
\item Support Vector Machines (SVM)
\item Random Forests
\item Adaboost
\item Classification Trees
\end{itemize}


<<echo=FALSE>>=
library(ggplot2)
library(gridExtra)
library(MASS)
library(vioplot)

library(verification)
library(caret)
library(rpart)
library(e1071) # SVM
library(gbm) # GBM

hotel = read.csv("../Data/hotel_bookings.csv")
Hotel_data<- read.csv(file = "../Data/hotelchanged.csv")
@

<<echo=FALSE>>=
hotel1 = subset(na.omit(hotel), select = -c(country, agent, company, reservation_status, reservation_status_date))
@


<<echo=FALSE>>=
kappa=function(x){
      n=sum(x)
      pobs=(x[1,1]+x[2,2])/n
      pexp=(sum(x[1,])*sum(x[,1])+sum(x[2,])*sum(x[,2]))/n^2
      kappa=(pobs-pexp)/(1-pexp)
      t1=0
      t2=0
      t3=0
      pii=x/n
      pidot=apply(pii,1,sum)
      pdotj=apply(pii,2,sum)
      for(i in 1:2){
            t1 = t1 + pii[i,i]*((1-pexp) - (1-pobs)*(pidot[i]+pdotj[i]))^2
      }
      t2 = pii[1,2]*(pdotj[1]+pidot[2])^2 + pii[2,1]*(pdotj[2] + pidot[1])^2
      t3 = (pobs*pexp-2*pexp+pobs)^2
      vhat = (t1 + t2*(1-pobs)^2 -t3)/(n*(1-pexp)^4)
      se=sqrt(vhat)
      return(c(kappa,se))
}


class.sum=function(truth,predicted){
     xt=table(truth,round(predicted+0.000001))
     pcc=round(100*sum(diag(xt))/sum(xt),2)
     spec=round(100*xt[1,1]/sum(xt[1,]),2)
     sens=round(100*xt[2,2]/sum(xt[2,]),2)
     kap=round(kappa(xt)[1],4)
     au=round(roc.area(truth,predicted)$A,4)
     return(cbind(c("Percent Correctly Classified = ","Specificity = ","Sensitivity = ","Kappa =","AUC= "),c(pcc,spec,sens,kap,au)))
     }
@


\item \underline{\bf Classification Trees:}

To prepare the data for the classification tree we subset the data and removed the following
columns from the data set: country, agent, company, reservation status, reservation status date.


\item \underline{\bf Untuned Support Vector Machines:}

<<echo=FALSE>>=
set.seed(424)
hotel_samp = hotel[sample(nrow(hotel), 4000), ]
data = hotel_samp

hotel.svm.xvalpred=rep(0,nrow(data))
xvs=rep(1:10,length=nrow(data))
xvs=sample(xvs)
for(i in 1:10){
      train=data[xvs!=i,]
      test=data[xvs==i,]
      glub=svm(as.factor(is_canceled)~ . - reservation_status,probability=TRUE,data=train)
      hotel.svm.xvalpred[xvs==i]=attr(predict(glub,test,probability=TRUE),"probabilities")[,2]
}

table(data$is_canceled,round(hotel.svm.xvalpred))
class.sum(data$is_canceled,hotel.svm.xvalpred)
@


~ \newpage

\item \underline{\bf Untuned Gradient Boosting Machines:}

<<echo=FALSE>>=
set.seed(424)
hotel.gbm.xvalpr=rep(0,nrow(data))
xvs=rep(1:10,length=nrow(data))
xvs=sample(xvs)
for(i in 1:10){
      train=data[xvs!=i,]
      test=data[xvs==i,]
      glub=gbm(is_canceled~ . - reservation_status,distribution="bernoulli",n.trees=5000,data=train)
      hotel.gbm.xvalpr[xvs==i]=predict(glub,newdata=test,type="response",n.trees=5000)
}

table(data$is_canceled,round(hotel.gbm.xvalpr))
class.sum(data$is_canceled,hotel.gbm.xvalpr)
@

\item \underline{\bf Untuned Gradient Boosting Machines:}

\end{enumerate}


\newpage


\noindent{\Large \bf General Instructions}~\\



\end{document}

