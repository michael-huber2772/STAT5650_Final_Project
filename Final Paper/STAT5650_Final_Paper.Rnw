\documentclass[12pt,letterpaper,final]{article}

\usepackage{Sweave}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage{textcomp}
\usepackage{wasysym}

\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.15in}
%\setlength{\topmargin}{0.5in}
\setlength{\textheight}{22cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parskip}{5pt plus 2pt minus 3pt}

\def\thefootnote{\fnsymbol{footnote}}
\setcounter{footnote}{1}

\renewcommand{\baselinestretch}{1.2}
\renewcommand{\labelenumi}{(\roman{enumi})}

\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}
\renewcommand{\floatpagefraction}{1.0}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}

% to get nice proofs ...
\newcommand{\qedsymb}{\mbox{ }~\hfill~{\rule{2mm}{2mm}}}
\newenvironment{proof}{\begin{trivlist}
\item[\hspace{\labelsep}{\bf\noindent Proof: }]
}{\qedsymb\end{trivlist}}


\newfont{\msymb}{cmsy10 scaled 1000}

\def\nullset{\mbox{\O}}
\def\R{{I\!\!R}}
\def\C{{I\!\!\!\!C}}
\def\N{{I\!\!N}}

\def\P{\mbox{\msymb P}}


%\parskip 0.1in
\pagenumbering{arabic}    %  Start using 1,2,... as page numbers.
\pagestyle{plain}         %  Page numbers in middle bottom of page.
%\setcounter{page}{80}  % XXXXXXXXXXXXXXXXX
%\setcounter{theorem}{5} % XXXXXXXXXXXXXXXXX
%\setcounter{definition}{10} % XXXXXXXXXXXXXXXXX

\parindent 0in


\begin{document}

\SweaveOpts{concordance=TRUE}

\begin{table}\centering
\begin{tabular*}{6.15in}{@{\extracolsep{\fill}}|llr|} \hline
STAT 5650 Statistical Learning and Data Mining 1 & \hspace*{0.5 in} & Spring 2020 \\
 & & \\
\multicolumn{3}{|c|}{
{\bf Names:} Greg Hoffmann, Kristen Sohm, Michael Huber, Varsha Reddy Mandadi} \\
 & & \\
\multicolumn{3}{|c|}{
{\bf Submission Date:} 04/22/2020} \\
 & & \\
\multicolumn{3}{|c|}{
Final Project: Applying Prediction Methods to Hotel Data } \\
 & & \\
\multicolumn{3}{|c|}{
200 Points --- Due Friday 10/18/2019 (via Canvas by 11:59pm)} \\
\hline
\end{tabular*}
\end{table}


~ \newpage

\begin{enumerate}

\item \underline{\bf Prediction Methods:}
After working with the data and reviewing the notes from Dr. Cutler on our project proposal
we had to edit what methods of prediction we were going to apply against the data. Below is
a list of methods that we were able to get to run against our hotel data set.

\begin{itemize}
\item Gradient Boosting Machines (GBM)
\item Support Vector Machines (SVM)
\item Random Forests
\item Adaboost
\item Classification Trees
\end{itemize}


<<echo=FALSE>>=
library(ggplot2)
library(gridExtra)
library(MASS)
library(vioplot)

library(verification)
library(caret)
library(e1071) # SVM
library(gbm) # GBM

hotel = read.csv("../Data/hotel_bookings.csv")
@


<<echo=FALSE>>=
kappa=function(x){
      n=sum(x)
      pobs=(x[1,1]+x[2,2])/n
      pexp=(sum(x[1,])*sum(x[,1])+sum(x[2,])*sum(x[,2]))/n^2
      kappa=(pobs-pexp)/(1-pexp)
      t1=0
      t2=0
      t3=0
      pii=x/n
      pidot=apply(pii,1,sum)
      pdotj=apply(pii,2,sum)
      for(i in 1:2){
            t1 = t1 + pii[i,i]*((1-pexp) - (1-pobs)*(pidot[i]+pdotj[i]))^2
      }
      t2 = pii[1,2]*(pdotj[1]+pidot[2])^2 + pii[2,1]*(pdotj[2] + pidot[1])^2
      t3 = (pobs*pexp-2*pexp+pobs)^2
      vhat = (t1 + t2*(1-pobs)^2 -t3)/(n*(1-pexp)^4)
      se=sqrt(vhat)
      return(c(kappa,se))
}


class.sum=function(truth,predicted){
     xt=table(truth,round(predicted+0.000001))
     pcc=round(100*sum(diag(xt))/sum(xt),2)
     spec=round(100*xt[1,1]/sum(xt[1,]),2)
     sens=round(100*xt[2,2]/sum(xt[2,]),2)
     kap=round(kappa(xt)[1],4)
     au=round(roc.area(truth,predicted)$A,4)
     return(cbind(c("Percent Correctly Classified = ","Specificity = ","Sensitivity = ","Kappa =","AUC= "),c(pcc,spec,sens,kap,au)))
     }
@



\item \underline{\bf Untuned Support Vector Machines:}

<<echo=FALSE>>=
# set.seed(424)
# hotel_samp = hotel[sample(nrow(hotel), 4000), ]
# data = hotel_samp
# 
# hotel.svm.xvalpred=rep(0,nrow(data))
# xvs=rep(1:10,length=nrow(data))
# xvs=sample(xvs)
# for(i in 1:10){
#       train=data[xvs!=i,]
#       test=data[xvs==i,]
#       glub=svm(as.factor(is_canceled)~ . - reservation_status,probability=TRUE,data=train)
#       hotel.svm.xvalpred[xvs==i]=attr(predict(glub,test,probability=TRUE),"probabilities")[,2]
# }
# 
# table(data$is_canceled,round(hotel.svm.xvalpred))
# class.sum(data$is_canceled,hotel.svm.xvalpred)
@


\item \underline{\bf Untuned Gradient Boosting Machines:}

<<echo=FALSE>>=
# set.seed(424)
# hotel.gbm.xvalpr=rep(0,nrow(data))
# xvs=rep(1:10,length=nrow(data))
# xvs=sample(xvs)
# for(i in 1:10){
#       train=data[xvs!=i,]
#       test=data[xvs==i,]
#       glub=gbm(is_canceled~ . - reservation_status,distribution="bernoulli",n.trees=5000,data=train)
#       hotel.gbm.xvalpr[xvs==i]=predict(glub,newdata=test,type="response",n.trees=5000)
# }
# 
# table(data$is_canceled,round(hotel.gbm.xvalpr))
# class.sum(data$is_canceled,hotel.gbm.xvalpr)
@



\newpage


\item \underline{\bf Question 2:}
<Description of Question 2>


\begin{enumerate}
\item (6 Points) Recreate the graphs (and layout) below using baseR.
Use a ruler to  check that the width and height proportions in your graphs
match the ones I have used. I worked with integer multiples!
Include your R code and the resulting graphs.
Hint: You can create a new line via \verb|\n| without any extra spaces before/after \verb|\n|.

<<fig=TRUE>>=
grid <- matrix(c(1, 1, 1, 2, 2, 3, 3, 3, 4, 4), 
              nrow = 2, ncol = 5, byrow = TRUE)

layout(grid)

par(mar = c(4, 3, 4, 2))
boxplot(geyser$duration,
        horizontal = TRUE,
        main = "Old Faithful:\n Duration",
        ylim = c(0, 6))
hist(geyser$duration,
     main = "Old Faithful",
     xlab = "Duration",
     ylab = "Count",
     xlim = c(0, 6))
boxplot(geyser$waiting,
        horizontal = TRUE,
        main = "Old Faithful:\n Waiting",
        ylim = c(40, 120))
hist(geyser$waiting,
     main = "Old Faithful",
     xlab = "Waiting",
     ylab = "Count",
     xlim = c(40, 120),
     ylim = c(0, 100),
     breaks = seq(40, 120, by = 10))
@
\underline{Refernces:}
\begin{itemize}
\item https://www.statmethods.net/advgraphs/layout.html
\item https://stackoverflow.com/questions/31319942/change-the-size-of-a-plot-when-plotting-multiple-plots-in-r
\item https://www.youtube.com/watch?v=Z3V4Pbxeahg
\end{itemize}




\newpage


\item (2 Points) Recreate the graph below using ggplot2.
Include your R code and the resulting graph.

<<fig=TRUE>>=
ggplot(geyser, aes(x=duration, y=waiting)) +
  geom_point() +
  xlab("Duration") +
  ylab("Waiting") +
  xlim(0, 6) +
  ylim(40, 120) +
  ggtitle("Old Faithful Data") +
  theme(plot.title = element_text(hjust = 0.5))
@




\newpage


\item (2 Points) Doesn't the scatterplot in (b) above look rather different 
than the scatterplot in Question 1 (j)? Note that the help page for {\it geyser}
states \verb|waiting	 numeric	 Waiting time for this eruption| and \\
\verb|The waiting time was incorrectly described as the time to the next|
\verb|eruption in the original files, and corrected for MASS version 7.3-30.| \\
Use this information to create a basic scatterplot 
for the {\it geyser} data that matches the overall 
appearance in Question 1 (j). 
Include your R code and the resulting graph.
No need to refine this scatterplot.

<<fig=TRUE>>=
duration <- geyser$duration[1:298]
waiting <- geyser$waiting[2:299]


df <- data.frame(duration, waiting)

ggplot(df, aes(x = duration, y = waiting )) + 
         geom_point()
@

\underline{Answer:} 
The geyser data appears to have a negative correlation with three clusters where the faithful data has a positive correlation with 2 clusters.


\end{enumerate}

\end{enumerate}


\newpage


\noindent{\Large \bf General Instructions}~\\

\begin{enumerate}

\item <Instruction 1>

\item <Instruction 2>

\end{enumerate}


\end{document}

